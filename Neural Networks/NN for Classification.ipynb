{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Activation Functions and their Transfer Derivatives\n",
    "\n",
    "# 1. Sigmoid / Logistic Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "# 2. Rectified Linear Unit Function\n",
    "def relu(x):\n",
    "     return abs(x) * (x > 0)\n",
    "\n",
    "def drelu(x):\n",
    "     return 1. * (x > 0.)\n",
    "\n",
    "# 3. Leaky-Relu Functions\n",
    "def lrelu(x):\n",
    "    return np.where(x > 0., x, x * 0.01)\n",
    "\n",
    "def dlrelu(x):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0.] = 0.01\n",
    "    return dx\n",
    "\n",
    "# 4. Hyperbolic Tan Function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1.0 - (np.power(np.tanh(x),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_forward(data_in, w0,w1,w2, w3, b0,b1,b2,b3):\n",
    "    '''\n",
    "    The Feed-forward considers 5 layers including input and output layer.\n",
    "    \n",
    "    The output layer/neuron is a classification node.\n",
    "    \n",
    "    returns: state of each layer\n",
    "    '''\n",
    "    layer0 = data_in\n",
    "    layer1 = tanh(np.dot(layer0, w0)+b0)\n",
    "    layer2 = tanh(np.dot(layer1, w1)+b1)\n",
    "    layer3 = tanh(np.dot(layer2, w2)+b2)\n",
    "    layer4 = sigmoid(np.dot(layer3, w3)+b3)\n",
    "\n",
    "    return layer0, layer1, layer2, layer3, layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropogate(i, layer0, layer1, layer2, layer3, layer4, actual_y, w0,w1,w2,w3,b0,b1,b2,b3, learning_rate):\n",
    "    '''\n",
    "    This backpropogate is only slightly different from a regular classifier\n",
    "    in ways in which the output layer gradient is calculated.\n",
    "    \n",
    "    Since the output layer is not a function of any activation function,\n",
    "    the delta doesn't need to be multiplied with the transfer derivative of the\n",
    "    output layer.\n",
    "    \n",
    "    The rest is all the same.\n",
    "    \n",
    "    returns: weights and bias matrices\n",
    "    '''\n",
    "    l4_error = layer4 - actual_y\n",
    "    l4_delta = l4_error * dsigmoid(layer4)\n",
    "    dh4 = np.dot(layer3.T, l4_delta)\n",
    "    \n",
    "    l3_error = l4_delta.dot(w3.T)\n",
    "    l3_delta = l3_error * dtanh(layer3)\n",
    "    dh3 = np.dot(layer2.T, l3_delta)\n",
    "    \n",
    "    l2_error = l3_delta.dot(w2.T)\n",
    "    l2_delta = l2_error * dtanh(layer2)\n",
    "    dh2 = np.dot(layer1.T, l2_delta)\n",
    "    \n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * dtanh(layer1)\n",
    "    dh1 = np.dot(layer0.T, l1_delta)\n",
    "    \n",
    "    w3 = w3 - (learning_rate * dh4)\n",
    "    w2 = w2 - (learning_rate * dh3)\n",
    "    w1 = w1 - (learning_rate * dh2)\n",
    "    w0 = w0 - (learning_rate * dh1)\n",
    "    \n",
    "    b3 = b3 - (learning_rate * np.mean(l4_delta))\n",
    "    b2 = b2 - (learning_rate * np.mean(l3_delta))\n",
    "    b1 = b1 - (learning_rate * np.mean(l2_delta))\n",
    "    b0 = b0 - (learning_rate * np.mean(l1_delta))    \n",
    "   \n",
    "    if i%10==0 and (i!=0):\n",
    "        loss = np.mean(np.power(layer4-actual_y, 2))\n",
    "        loss_curve.append(loss)\n",
    "        iters.append(int(i))\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\"\\n\", int(i), loss)\n",
    "\n",
    "        \n",
    "    return w0, w1,w2,w3, b0,b1,b2,b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(testx, testy):\n",
    "    correct = 0\n",
    "    layer0, layer1, layer2, layer3, layer4 = feed_forward(testx,w0, w1,w2,w3, b0,b1,b2,b3)\n",
    "    for i in range(len(testx)):\n",
    "        if np.argmax(layer4[i]) == np.argmax(testy[i]):\n",
    "            correct +=1 \n",
    "            \n",
    "    return f\"Accuracy: {correct*100/len(testy)}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working on Wine Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "features = wine.data\n",
    "target = wine.target\n",
    "\n",
    "nt = []\n",
    "for i in target:\n",
    "    op = [0,0,0]\n",
    "    op[i] = 1\n",
    "    nt.append(op)\n",
    "    \n",
    "target = np.array(nt)\n",
    "\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "Y = target\n",
    "\n",
    "X = (X-X.min()) / (X.max()-X.min())\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X.values,Y, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "w0 = np.random.random((13,50))\n",
    "w1 = np.random.random((50,30))\n",
    "w2 = np.random.random((30, 5))\n",
    "w3 = np.random.random((5,3))\n",
    "\n",
    "b0 = np.random.random((1,1))-1\n",
    "b1 = np.random.random((1,1))-1\n",
    "b2 = np.random.random((1,1))-1\n",
    "b3 = np.random.random((1,1))-1\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising variables to track loss vs iterations so we can plot the changes\n",
    "loss_curve = []\n",
    "iters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee0c30707c4417b8c1d341cdbc9d462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100 0.13577025440630552\n",
      "\n",
      " 200 0.08943909172475077\n",
      "\n",
      " 300 0.034659547756674555\n",
      "\n",
      " 400 0.02531048589905879\n",
      "\n",
      " 500 0.020604561684376133\n",
      "\n",
      " 600 0.016363282737875903\n",
      "\n",
      " 700 0.01083046715743548\n",
      "\n",
      " 800 0.006881278183141215\n",
      "\n",
      " 900 0.004857799374374131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(epochs)):\n",
    "    layer0, layer1, layer2, layer3, layer4 = feed_forward(xtrain, w0,w1,w2, w3, b0,b1,b2,b3)\n",
    "    w0,w1,w2, w3, b0,b1,b2,b3 = backpropogate(i,layer0, layer1, layer2, layer3, layer4, ytrain, w0,w1,w2, w3, b0,b1,b2,b3, 0.005 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x259f1f7b3c8>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x259f1f379e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iters, loss_curve,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 100.0%'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 91.60839160839161%'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
